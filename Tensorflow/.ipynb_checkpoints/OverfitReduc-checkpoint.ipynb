{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "# data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "# assumes that input data is a row vector([1, 2, 3 ...])\n",
    "def add_layer(data_input, num_inputs, num_neurons, activation_function=None, keep_prob=1.0,\n",
    "              has_dims=False):\n",
    "    weights = tf.Variable(tf.random_normal([num_neurons, num_inputs])) \n",
    "    biases = tf.Variable(tf.random_normal([num_neurons, 1]))  # one bias per neuron\n",
    "    output = tf.add(tf.matmul(weights, data_input), biases)   # broadcast addition\n",
    "    dropout_output = tf.nn.dropout(output, keep_prob)\n",
    "    if activation_function:\n",
    "        if has_dims == True:\n",
    "            return activation_function(output, dim=0)\n",
    "        return activation_function(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define placeholder for the data\n",
    "X_input = tf.placeholder(tf.float32, [784, None])\n",
    "Y_input = tf.placeholder(tf.float32, [10, None])\n",
    "dropout_prob = tf.placeholder(tf.float32)\n",
    "learning_rate = 0.01\n",
    "activation_function1, activation_function2 = tf.nn.tanh, tf.nn.softmax\n",
    "num_neurons_h1, num_neurons_h2 = 10, 10\n",
    "hidden_layer_1 = add_layer(X_input, 784, num_neurons_h1, activation_function1, dropout_prob)\n",
    "prediction = add_layer(hidden_layer_1, num_neurons_h1, num_neurons_h2,\n",
    "                                       activation_function2, dropout_prob, has_dims=True)\n",
    "# using cross entropy loss -> tf does a sum to reduce the loss vector to a value\n",
    "loss = -1 * tf.reduce_sum(Y_input * tf.log(prediction), 0)\n",
    "train_func = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1112\n",
      "0.1913\n",
      "0.2294\n",
      "0.2652\n",
      "0.2857\n",
      "0.3046\n",
      "0.3239\n",
      "0.3272\n",
      "0.3358\n",
      "0.3413\n",
      "0.3471\n",
      "0.3524\n",
      "0.353\n",
      "0.3573\n",
      "0.3611\n",
      "0.3643\n",
      "0.3656\n",
      "0.3652\n",
      "0.3639\n",
      "0.365\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as session:\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    batch_size = 100\n",
    "    iterations = 200\n",
    "    batch_input, batch_output = mnist.train.next_batch(batch_size=batch_size)\n",
    "    batch_input, batch_output = np.transpose(batch_input), np.transpose(batch_output)\n",
    "    test_images, test_labels = np.transpose(mnist.test.images), np.transpose(mnist.test.labels)\n",
    "    for iteration in range(iterations):\n",
    "        session.run(train_func, feed_dict={X_input: batch_input,\n",
    "                                           Y_input: batch_output, dropout_prob: 1})\n",
    "        if iteration % 10 == 0:\n",
    "            current_pred = session.run(prediction, feed_dict={\n",
    "                                                    X_input: test_images, dropout_prob: 1.0})\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(current_pred, 0),\n",
    "                                        tf.argmax(test_labels, 0)), tf.float32))\n",
    "            print(session.run(accuracy))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    x = tf.nn.softmax(np.array([[1, 2, 3], [4, 5, 6]], dtype=float), dim=0)\n",
    "    print(session.run(x))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
